\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{float}

% Page margins
\geometry{margin=1in}

\title{\textbf{Single and Multi-Agent Reinforcement Learning in Double Snake}}
\author{CS329 RL Project}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This project implements and evaluates various Multi-Agent Reinforcement Learning (MARL) algorithms in a grid-based Snake environment. We explore two distinct modes: a \textbf{Competitive} zero-sum scenario using Nash Q-Learning and Deep Nash Q-Learning, and a \textbf{Cooperative} scenario utilizing DQN, Double DQN (DDQN), SARSA, Monte Carlo, and Tabular Q-Learning. We analyze the stability, convergence, and win/survival rates of these agents. Results indicate that Deep Nash Q-Learning successfully approximates equilibrium strategies in high-dimensional state spaces, while DDQN provides the most stable performance in the cooperative setting with an average evaluation score of 12.89.
\end{abstract}

\section{Introduction}
Reinforcement Learning (RL) \cite{sutton1998reinforcement} in multi-agent environments presents unique challenges, particularly the non-stationarity of the environment as agents update their policies simultaneously. This project applies RL to a two-player Snake game. The project is divided into two domains:
\begin{enumerate}
    \item \textbf{Competitive:} Agents compete for food, where one agent's gain is the other's loss.
    \item \textbf{Cooperative:} The Agent, controlling both the snakes, attempts to survive and maximise score within the same grid, navigating around each other.
\end{enumerate}

\section{Environment and Problem Formulation}

\subsection{State Space}
The environment represents the game state differently for the two modes to optimize learning:
\begin{itemize}
    \item \textbf{Competitive Mode:} 2 agents, each having a feature vector of size 18. It includes danger detection (straight, left, right), current direction, relative food direction, relative opponent head direction, and a length comparison flag.
    \item \textbf{Cooperative Mode:} 1 agent with a feature vector of size 22. It includes expanded danger detection that accounts for the specific location of the other agent's body segments to prevent collisions. Controls both the snakes.
\end{itemize}

\subsection{Action Space}
The agents operate in a discrete action space $A = \{0, 1, 2\}$, representing relative direction changes:
\begin{itemize}
    \item 0: Continue Straight
    \item 1: Turn Left
    \item 2: Turn Right
\end{itemize}
In the Deep Nash implementations, the network outputs joint action values ($3 \times 3 = 9$ pairs) to compute the equilibrium.

\subsection{Reward Structure}
\begin{itemize}
    \item \textbf{Food Reward:} +10 for eating.
    \item \textbf{Death Penalty:} -10 for collision with walls, self, or opponent.
    \item \textbf{Survival:} Small negative reward (-0.1) or 0 to encourage active food seeking.
    \item \textbf{Opponent Death (Competitive):} +5 bonus if the opponent dies.
\end{itemize}

\section{Methodology}

\subsection{Competitive Algorithms}
\subsubsection{Nash Q-Learning}
We extended tabular Q-learning \cite{watkins1992q} to a general-sum game, following the Nash Q-Learning framework established by Hu and Wellman \cite{hu2003nash}. At each step, instead of maximizing individual Q-values, the agent solves a Linear Programming (LP) problem to find the Nash Equilibrium of the stage game defined by the joint Q-values $Q(s, a^1, a^2)$. The `scipy.optimize.linprog` library is used to solve for the maxmin strategy.

\subsubsection{Deep Nash Q-Learning}
To handle the continuous nature of the game flow and larger effective state spaces, we implemented a Deep Neural Network (DNN) to approximate the Q-values, leveraging the architecture of Deep Q-Networks \cite{mnih2015human} adapted for multi-agent equilibrium computation.
\begin{itemize}
    \item \textbf{Architecture:} Fully connected layers (Input $\to$ 128 $\to$ 128 $\to$ 9).
    \item \textbf{Training:} A replay buffer of capacity 50,000 is used. The target value is calculated based on the Nash value of the next state's Q-matrix.
\end{itemize}

\subsection{Cooperative/Single-Agent Algorithms}
We benchmarked several standard algorithms, each tailored to address specific challenges in cooperative multi-agent reinforcement learning:

\begin{itemize}
    \item \textbf{SARSA (On-Policy):} Proposed by Rummery and Niranjan \cite{rummery1994line}, this tabular method updates the Q-value based on the current action and the next action taken by the agent. It is on-policy, meaning it evaluates and improves the policy that is used to make decisions. SARSA is suitable for environments where the agent needs to learn from its current behavior.

    \item \textbf{Q-Learning (Off-Policy):} Introduced by Watkins and Dayan \cite{watkins1992q}, Q-Learning is off-policy and updates the Q-value based on the maximum possible reward from the next state, regardless of the agent's actual next action. This makes it more exploratory and often faster to converge in deterministic environments.

    \item \textbf{First-Visit Monte Carlo:} This method estimates the value of a state-action pair by averaging the returns of the first visit to that pair in an episode. It is particularly useful in episodic tasks where the agent can learn from complete trajectories.

    \item \textbf{Deep Q-Network (DQN):} Mnih et al. \cite{mnih2015human} extended Q-Learning by using a neural network to approximate the Q-values, enabling it to handle high-dimensional state spaces. DQN employs experience replay and a target network to stabilize training.

    \item \textbf{Double DQN (DDQN):} Proposed by Van Hasselt et al. \cite{van2016deep} as an improvement over DQN, DDQN decouples the action selection from the Q-value evaluation to reduce overestimation bias. This results in more stable and reliable learning, especially in environments with noisy rewards.
\end{itemize}

\section{Experiments and Results}

\subsection{Hyperparameters}
Table \ref{tab:hyperparams} summarizes the configuration used during the final training runs.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
    \toprule
    Parameter & Competitive (Deep Nash) & Cooperative (DQN/DDQN) \\
    \midrule
    Episodes & 3,000 - 5,000 & 20,000 \\
    Learning Rate ($\alpha$) & 0.001 & 0.001 \\
    Discount ($\gamma$) & 0.95 & 0.99 \\
    Batch Size & 64 & 64 \\
    Buffer Size & 50,000 & 20,000 \\
    Epsilon Decay & 0.995 & 0.995 \\
    Hidden Layers & 128 & 256 \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for Deep Learning models.}
    \label{tab:hyperparams}
\end{table}

\subsection{Competitive Analysis}
Training logs for Nash Q-Learning and Deep Nash Q-Learning indicate a high prevalence of "Draws".
\begin{itemize}
    \item \textbf{Deep Nash Q Results (100 test episodes):}
    \begin{itemize}
        \item Agent 1 Wins: 3\%
        \item Agent 2 Wins: 0\%
        \item Draws: 97.0\%
    \end{itemize}
    \item \textbf{Tabular Nash Q Results  (100 test episodes):}
    \begin{itemize}
        \item Agent 1 Wins: 15.0\%
        \item Agent 2 Wins: 50.0\%
        \item Draws: 35.0\%
    \end{itemize}
\end{itemize}
The high draw rate suggests the Deep Nash Q strategy in this specific environment tends towards conservative survival rather than aggressive killing, likely due to the high penalty for mutual death. The lower draw rate in Nash Q stratergy could be due to the fact that the tabular method learns exact state-action values and therefore converges more cleanly to decisive, exploitative strategies, whereas deep Nash Q generalizes conservatively and over-penalizes risky aggression. The tabular agents thus more readily identify profitable attack opportunities and engage in high-stakes interactions during training, resulting in both wins and losses, whereas the deep agents drift towards mutually safe, non-engaging behavior that prioritizes survival and thus produces disproportionately many draws.

\subsection{Cooperative Analysis}
The Deep Learning approaches significantly outperformed tabular methods due to the state space complexity.
\begin{itemize}
    \item \textbf{DQN Performance:} Achieved an average score of 15.89 with a maximum score of 35 in evaluation.
    \item \textbf{DDQN Performance:} Achieved an average score of 12.89 with a maximum of 37. While the average was slightly lower than DQN, training logs suggest DDQN exhibited more stable loss convergence.
\end{itemize}

\subsection{Comparative Analysis}

The competitive and cooperative approaches in this project highlight distinct challenges and outcomes in multi-agent reinforcement learning:

\begin{itemize}
    \item \textbf{Objectives:} Competitive algorithms aim to maximize individual rewards in a zero-sum setting, often leading to conservative strategies to avoid mutual losses. Cooperative algorithms, on the other hand, focus on maximizing collective rewards, requiring agents to balance individual gains with team objectives.

    \item \textbf{Challenges:} Competitive algorithms, particularly Nash Q-Learning, face computational challenges in solving Nash equilibria at each state, which can be prohibitive in real-time scenarios. Cooperative algorithms, especially deep learning-based methods like DQN and DDQN, struggle with stability and convergence in high-dimensional state spaces but benefit from experience replay and target networks.

    \item \textbf{Outcomes:} Competitive strategies often result in high draw rates, reflecting a tendency towards survival rather than aggressive play. Cooperative strategies, particularly DDQN, demonstrate robust performance with higher average scores and more stable training dynamics.

    \item \textbf{Scalability:} Deep learning-based approaches in both domains show better scalability to larger state spaces compared to tabular methods, making them more suitable for complex environments.
\end{itemize}


\subsection{Evaluation Results}

Table \ref{tab:eval_results} summarizes the evaluation results for all models, including both competitive and cooperative algorithms. Metrics such as average score, maximum score, and average episode length are reported.

\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Algorithm} & \textbf{Avg. Score} & \textbf{Max Score} & \textbf{Min Score} & \textbf{Avg. Episode Length} \\
    \midrule
    Deep Nash Q-Learning & $0.03 \pm 0.17$ & 0 & 0 & $655.0 \pm 113.6$ \\
    Nash Q-Learning & $3.67 \pm 2.83$ & 15 & 0 & $92.1 \pm 75.4$ \\
    Double DQN (DDQN) & $12.89 \pm 7.49$ & 37 & 1 & $130.38 \pm 78.08$ \\
    DQN & $15.89 \pm 7.52$ & 35 & 0 & $164.74 \pm 79.13$ \\
    Monte Carlo (On-Policy) & $6.15 \pm 2.74$ & 13 & 0 & $77.50 \pm 37.88$ \\
    Q-Learning & $8.09 \pm 3.66$ & 17 & 1 & $135.20 \pm 77.87$ \\
    SARSA (On-Policy) & $1.86 \pm 1.79$ & 7 & 0 & $285.32 \pm 165.97$ \\
    \bottomrule
    \end{tabular}
    \caption{Evaluation results for all models. Metrics include average score, maximum score, minimum score, and average episode length.}
    \label{tab:eval_results}
\end{table}

\section{Supporting Figures}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/deep_nash_training_plot_final.png}
    \caption{Deep Nash Q-Learning (Final loss (Avg.): \textbf{0.291})}
    \label{fig:deep_nash_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/nash_q_training_plot_final.png}
    \caption{Tabular Nash Q-Learning}
    \label{fig:nash_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/dqn_training_plot_final.png}
    \caption{Cooperative DQN}
    \label{fig:dqn_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ddqn_training_plot_final.png}
    \caption{Cooperative DDQN}
    \label{fig:ddqn_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/qlearning_training_plot_final.png}
    \caption{Tabular Q-Learning}
    \label{fig:q_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sarsa_training_plot_final.png}
    \caption{SARSA (On-Policy)}
    \label{fig:sarsa_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/monte_carlo_training_plot.png}
    \caption{Monte Carlo (On-Policy)}
    \label{fig:mc_loss}
\end{figure}

\section{Contributions}

\begin{enumerate}
    \item Aarsh Wankar: Deep Q and double deep q and results for corporative. Also organized the repo :D.
    \item Abhinav Khot: Monte Carlo and SARSA algorithm implementation. Also organized the repo ;).
    \item Jaskirat Singh Maskeen: Training Models, Competitive results, Organising the cluttered repository :), and Report.
    \item Karan Sagar Gandhi: Implemented Nash Q, Deep Nash Q and Q learning algorithms. Also implemented the Game and rendering logic for the setup.
\end{enumerate}

\section{Conclusion}
This project demonstrated the application of MARL to Snake. In the competitive domain, Nash Q-Learning successfully identified safe strategies, resulting in prolonged survival (draws) rather than quick eliminations. In the cooperative domain, Deep Q-Learning methods proved robust, with DQN and DDQN agents learning to navigate the grid and consume food effectively while avoiding dynamic obstacles (the other agent).

Future work could involve implementing Minimax-Q for strictly zero-sum scenarios or adding communication channels between cooperative agents to improve collision avoidance in tighter grid spaces.

\begin{thebibliography}{9}

\bibitem{sutton1998reinforcement}
Sutton, R. S., \& Barto, A. G. (2018).
\textit{Reinforcement Learning: An Introduction}.
MIT press.

\bibitem{watkins1992q}
Watkins, C. J., \& Dayan, P. (1992).
Q-learning.
\textit{Machine learning}, 8(3), 279-292.

\bibitem{hu2003nash}
Hu, J., \& Wellman, M. P. (2003).
Nash Q-learning for general-sum stochastic games.
\textit{Journal of Machine Learning Research}, 4(Nov), 1039-1069.

\bibitem{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015).
Human-level control through deep reinforcement learning.
\textit{Nature}, 518(7540), 529-533.

\bibitem{van2016deep}
Van Hasselt, H., Guez, A., \& Silver, D. (2016).
Deep reinforcement learning with double q-learning.
\textit{Proceedings of the AAAI conference on artificial intelligence}, 30(1).

\bibitem{rummery1994line}
Rummery, G. A., \& Niranjan, M. (1994).
On-line Q-learning using connectionist systems.
\textit{University of Cambridge, Department of Engineering}.

\end{thebibliography}

\end{document}